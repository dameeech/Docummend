{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docummend Functions Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import gensim\n",
    "import pypandoc\n",
    "import pyLDAvis\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import gensim.corpora         as corpora\n",
    "import matplotlib.pyplot      as plt\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils         import datapath, common_texts\n",
    "\n",
    "from wordcloud        import WordCloud\n",
    "from nltk.corpus      import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize    import sent_tokenize, word_tokenize\n",
    "from nltk.stem        import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.neighbors               import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "pyLDAvis.enable_notebook()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list  = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += ['©', '§', 'v.', '(see', 'The', 'Court,', 'New', 'York', 'In', 'Dept.',\n",
    "            'A.D.3d', 'N.Y.S.2d', 'N.Y.S.3d', 'N.E.2d', 'N.Y.2d']\n",
    "sw_set   = set(sw_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make DB Function\n",
    "This function takes in a text file and does a simple scrub to extract the decision text from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db(file):\n",
    "    \n",
    "    f  = open(file, 'r', encoding='cp1252')\n",
    "    f1 = f.readlines()\n",
    "    \n",
    "    start  = 0\n",
    "    stop   = 0\n",
    "    d_list = []    \n",
    "    for line in f1[0:]:\n",
    "        stop += 1\n",
    "        if 'DECISION' in line:\n",
    "            start = stop\n",
    "        if 'End of Document\\n' in line:\n",
    "            d_list = f1[start:stop-1]\n",
    "            \n",
    "    f3 = []        \n",
    "    for x in d_list:\n",
    "        if x != '\\n':\n",
    "            f3.append(x[:-1])\n",
    "            \n",
    "    decision = ''\n",
    "    for x in f3:\n",
    "        if x != ' ':\n",
    "            decision += str(x)\n",
    "    \n",
    "    return decision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Train Data\n",
    "This function uses the previous function and performs it on the entire folder of train data. It then transforms the data into a dataframe and outputs it into a CSV file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_data(source, outputfile):\n",
    "    \n",
    "    source = source\n",
    "    os.chdir(source)\n",
    "\n",
    "    name_list     = []\n",
    "    decision_list = []\n",
    "\n",
    "    for file in glob.glob(\"*.txt\"):\n",
    "        name_list.append(file[:-4])\n",
    "        decision_list.append(make_db(file))\n",
    "\n",
    "    df1 = pd.DataFrame(name_list, columns=['Name'])\n",
    "    df2 = pd.DataFrame(decision_list, columns=['Decision'])\n",
    "    df  = pd.concat([df1, df2], axis=1)\n",
    "    \n",
    "    pd.DataFrame.to_csv(df, outputfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docx To Text Function\n",
    "This function takes in a .docx file and converts it into a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docx_to_text(source, outputfile):\n",
    "    \n",
    "    docxFilename = source\n",
    "    output = pypandoc.convert_file(docxFilename, 'plain', outputfile=outputfile)\n",
    "    assert output == \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Input Function\n",
    "This function takes in the target text file and cleans it down to just the Bill of Particulars text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(file):\n",
    "    \n",
    "    a  = open(file, 'r')\n",
    "    a1 = a.readlines()\n",
    "    \n",
    "    start  = 0\n",
    "    stop   = 0\n",
    "    i_list = []    \n",
    "    for line in a1[0:]:\n",
    "        stop += 1\n",
    "        if 'Plaintiff,' in line:\n",
    "            start = stop\n",
    "        if 'PLEASE' in line:\n",
    "            i_list = a1[start:stop-1]\n",
    "            \n",
    "    a3 = []        \n",
    "    for x in i_list:\n",
    "        if x != '\\n':\n",
    "            a3.append(x[:-1])\n",
    "            \n",
    "    file_input = ''\n",
    "    for x in a3:\n",
    "        if x != ' ':\n",
    "            file_input += str(x)\n",
    "            \n",
    "    return file_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Target Function\n",
    "This function takes in the target data text file and transforms it into a Pandas dataframe, which it then saves to a CSV file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_target(data, outputfile):\n",
    "    \n",
    "    input_doc = make_input(data)\n",
    "    columns   = ['Name', 'Text']\n",
    "    Name      = ['Drew Benasillo v. Upper East Side Pain Medicine']\n",
    "    Data      = {columns[1]:input_doc}\n",
    "    Target    = pd.DataFrame(Data, Name, columns=columns)\n",
    "\n",
    "    pd.DataFrame.to_csv(Target, outputfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Words Function\n",
    "This function takes in the decisions Pandas series and the stop words in order to prepare the data for LDA clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(decisions, stop_words=sw_list, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "\n",
    "    decisions     = [[word for word in doc.split() if word not in stop_words] for doc in decisions]\n",
    "    decisions_out = []\n",
    "    nlp           = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n",
    "    \n",
    "    for doc in decisions:\n",
    "        doc = nlp(\" \".join(doc)) \n",
    "        decisions_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        \n",
    "    decisions_out = [[word for word in doc if word not in stop_words] for doc in decisions_out]\n",
    "    \n",
    "    return decisions_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDAM Function\n",
    "This function takes in the training data and the amount of topics to create. It uses the process words function from above to vectorize it with spaCy to create an LDA Topics Map. There is an option to save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldam(data, topics, save=False):\n",
    "        \n",
    "    data_ready = process_words(data)\n",
    "    id2word    = corpora.Dictionary(data_ready)\n",
    "    corpus     = [id2word.doc2bow(text) for text in data_ready]\n",
    "    lda_model  = gensim.models.ldamodel.LdaModel(corpus       = corpus,\n",
    "                                                 id2word      = id2word,\n",
    "                                                 num_topics   = topics, \n",
    "                                                 random_state = 100,\n",
    "                                                 update_every = 1,\n",
    "                                                 chunksize    = 10,\n",
    "                                                 passes       = 10,\n",
    "                                                 alpha        = 'symmetric',\n",
    "                                                 iterations   = 100,\n",
    "                                                 per_word_topics = True)\n",
    "        \n",
    "    if save == True:\n",
    "        temp_file = datapath('/Users/dimitrybelozersky/Documents/Phase5/lda model results/lda_model')\n",
    "        lda_model.save(temp_file)\n",
    "    \n",
    "    vis = gensimvis.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "    \n",
    "    return vis, lda_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDAM Topics Function\n",
    "This function takes in the target data file and optionally the lda model results from the previous model(otherwise it uses the results saved from earlier) to show the topic most closely associated with the target file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldam_topics(target, new_model=False, model=False):\n",
    "        \n",
    "    temp_file = datapath('/Users/dimitrybelozersky/Documents/Phase5/lda model results/lda_model')\n",
    "    lda_model = gensim.models.ldamodel.LdaModel.load(temp_file)\n",
    "    \n",
    "    if new_model == True:\n",
    "        lda_model = model\n",
    "    \n",
    "    Target   = process_words(target)\n",
    "    T_dict   = corpora.Dictionary(Target)\n",
    "    T_corpus = [T_dict.doc2bow(text) for text in Target]\n",
    "\n",
    "    target_doc = T_corpus[0]\n",
    "    vector     = lda_model[target_doc]    \n",
    "    \n",
    "    topic_1_name  = '1: Negligence/Premises'\n",
    "    topic_1_match = vector[0][0][1]\n",
    "\n",
    "    topic_2_name  = '2: Medical Malpractice'\n",
    "    topic_2_match = vector[0][1][1]\n",
    "\n",
    "    X = [topic_1_name, topic_2_name]\n",
    "    Y = [topic_1_match, topic_2_match]\n",
    "    Y = np.array(Y)* 100\n",
    "\n",
    "    plt.bar(X,Y)\n",
    "    plt.title('Target Document Topic')\n",
    "    plt.xlabel('Topics')\n",
    "    plt.ylabel('Topic %')\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Train Data Function\n",
    "This function takes in a Pandas series and outputs the desired spaCy vectorization type into an output file as a CSV for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_train_data(data, spacy_type, outputfile):\n",
    "    \n",
    "    if spacy_type == 'md':\n",
    "        spacy_nlp       = spacy.load('en_core_web_md')\n",
    "    if spacy_type == 'lg':\n",
    "        spacy_nlp       = spacy.load('en_core_web_lg')\n",
    "    if spacy_type == 'trf':\n",
    "        spacy_nlp       = spacy.load('en_core_web_trf')\n",
    "    \n",
    "    df['Spacy']     = data.apply(lambda x: spacy_nlp(x))\n",
    "    Train_spacy_out = pd.DataFrame(np.vstack([x.vector for x in df.Spacy]))\n",
    "    pd.DataFrame.to_csv(Train_spacy_out, outputfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docummend Function\n",
    "The primary modeling function, it takes in the target data, the vectorizer it should use, and the amount of results desired. The output is the results in order of most similarity as well as the distance of each result from the target. Optionally it can output the results into a results folder, the topics that the target file is associated with, and an LDA clustering map based on the amount of desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docummend(target, vectorizer, results, out=False, Topics=False, LDA=False):\n",
    "    \n",
    "    if vectorizer == 'spacy':\n",
    "        Train_data      = pd.read_csv('data_csv/Train_spacy_out.csv', index_col=False)\n",
    "        Train_data      = Train_data.drop(['Unnamed: 0'], axis=1)\n",
    "        spacy_nlp       = spacy.load('en_core_web_md')\n",
    "        target['Spacy'] = target.Text.apply(lambda x: spacy_nlp(x))\n",
    "        Target_data     = pd.DataFrame(np.vstack([x.vector for x in target.Spacy]))\n",
    "        \n",
    "    if vectorizer == 'spacy lg':\n",
    "        Train_data      = pd.read_csv('data_csv/Train_spacy_out_lg.csv', index_col=False)\n",
    "        Train_data      = Train_data.drop(['Unnamed: 0'], axis=1)\n",
    "        spacy_nlp       = spacy.load('en_core_web_lg')\n",
    "        target['Spacy'] = target.Text.apply(lambda x: spacy_nlp(x))\n",
    "        Target_data     = pd.DataFrame(np.vstack([x.vector for x in target.Spacy]))       \n",
    "        \n",
    "    if vectorizer == 'spacy acc':\n",
    "        Train_data      = pd.read_csv('data_csv/Train_spacy_out_accuracy.csv', index_col=False)\n",
    "        Train_data      = Train_data.drop(['Unnamed: 0'], axis=1)\n",
    "        spacy_nlp       = spacy.load('en_core_web_trf')\n",
    "        target['Spacy'] = target.Text.apply(lambda x: spacy_nlp(x))\n",
    "        Target_data     = pd.DataFrame(np.vstack([x.vector for x in target.Spacy]))\n",
    "    \n",
    "    if vectorizer == 'tfidf':\n",
    "        Train_data  = pd.read_csv('data_csv/Decision_Database.csv', index_col=False)\n",
    "        Train_data  = Train_data.drop(['Unnamed: 0','length', 'Name'], axis=1)\n",
    "        tfidf_nlp   = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "        Train_data  = tfidf_nlp.fit_transform(Train_data['Decision'])\n",
    "        Target_data = tfidf_nlp.transform(target['Text'])\n",
    "        \n",
    "    if vectorizer == 'cv':\n",
    "        Train_data  = pd.read_csv('data_csv/Decision_Database.csv', index_col=False)\n",
    "        Train_data  = Train_data.drop(['Unnamed: 0','length', 'Name'], axis=1)     \n",
    "        cv_nlp      = CountVectorizer(stop_words=sw_set)\n",
    "        Train_data  = cv_nlp.fit_transform(Train_data['Decision'])\n",
    "        Target_data = cv_nlp.transform(target['Text'])\n",
    "    \n",
    "    neigh  = NearestNeighbors()\n",
    "    neigh.fit(Train_data)\n",
    "    result = neigh.kneighbors(Target_data, n_neighbors=results)\n",
    " \n",
    "    df = pd.read_csv('data_csv/Decision_Database.csv', index_col=False)\n",
    "    df = df.drop(['Unnamed: 0', 'length'], axis=1)\n",
    "    \n",
    "    distance       = [x for x in result[0].tolist()]\n",
    "    decision_index = [x for x in result[1].tolist()]\n",
    "    decision_name  = [df['Name'].iloc[x] for x in decision_index[0]]\n",
    "    decision_text  = [df['Decision'].iloc[x] for x in decision_index[0]]       \n",
    "    result_dict    = dict(zip(distance[0], decision_name))\n",
    "    percent_array  = np.array(distance) * 100\n",
    "    number         = 0\n",
    "    \n",
    "    print('\\nTop ' + str(results) + ' Recommended Decisions - ' + vectorizer + ':\\n')\n",
    "    for x in result_dict.values():\n",
    "        number += 1\n",
    "        print(str(number) + '.  '+ x)\n",
    "        \n",
    "    if vectorizer == 'spacy':\n",
    "        percent_array  = np.array(distance) * 100\n",
    "        plt.xlabel('% Difference From Target Document')\n",
    "    if vectorizer == 'spacy lg':\n",
    "        percent_array  = np.array(distance) * 100\n",
    "        plt.xlabel('% Difference From Target Document')\n",
    "    if vectorizer == 'tfidf':\n",
    "        percent_array  = np.array(distance)\n",
    "        plt.xlabel('Distance Difference From Target Document')\n",
    "    if vectorizer == 'cv':\n",
    "        percent_array  = np.array(distance)\n",
    "        plt.xlabel('Distance Difference From Target Document')\n",
    "        \n",
    "    plt.barh(decision_name, percent_array[0])\n",
    "    plt.title('Distance Of Results From Target Document')\n",
    "    plt.show()\n",
    "    \n",
    "    if out == True:\n",
    "        \n",
    "        doc_source  = '/Users/dimitrybelozersky/Documents/Phase5/all_decision_data_txt/'\n",
    "        dest        = '/Users/dimitrybelozersky/Documents/Phase5/nn_model_results/'\n",
    "        \n",
    "        for name in decision_name:\n",
    "            pattern = name + '.txt'\n",
    "            \n",
    "            files = glob.glob(doc_source + pattern)\n",
    "        \n",
    "            for file in files:\n",
    "                file_name = os.path.basename(file)\n",
    "                shutil.copy(file, dest + file_name)\n",
    "    \n",
    "    if Topics == True:\n",
    "        \n",
    "        ldam_topics(target.Text)\n",
    "        \n",
    "    if LDA == True:\n",
    "        \n",
    "        decision_df1  = pd.DataFrame(decision_name, columns=['Name'])\n",
    "        decision_df2  = pd.DataFrame(decision_text, columns=['Decision'])\n",
    "        decision_df   = pd.concat([decision_df1, decision_df2], axis=1)         \n",
    "        output, model = ldam(decision_df['Decision'], results)\n",
    "        return output\n",
    "    \n",
    "    else:    \n",
    "        pass \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
